name: TransformerCoT
base_dir: outputs
exp_dir: 2023-06-28-13:26:29
rng_seed: 0
mode: train
resume: true
num_gpu: 1
logger:
  name: wandb
debug:
  flag: false
  debug_size: 20
trainer: BaseTrainer
use_cot: true
dataset_wrapper: GPTWrapper
dataloader:
  batch_size: 128
  num_workers: 2
solver:
  gradient_accumulation_steps: 1
  grad_norm: 5.0
  epochs: 1
  epochs_per_eval: 10
  epochs_per_save: 0
  optim:
    name: AdamW
    args:
      lr: 0.0001
      betas:
      - 0.9
      - 0.98
  sched:
    name: warmup_cosine
    args:
      warmup_steps: 100
eval:
  name: BaseEvaluator
dataset:
  name: SCAN
  data_dir: ./datasets/SCAN
  subset: length
  length_split: 22
model:
  name: Roberta
  position_embedding_type: relative_key
  is_decoder: true
  vocab_size: 1024
  max_position_embeddings: 1024
  distance_clip: 10
  num_hidden_layers: 3
  num_attention_heads: 3
  hidden_size: 768
  intermediate_size: 3072
  hidden_dropout_prob: 0
  attention_probs_dropout_prob: 0
  max_length: 1024
  weight_decay: 0.01
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
