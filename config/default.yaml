defaults:
  - _self_
  - dataset: SCAN
  - model: Roberta

name: TransformerCoT
base_dir: outputs
exp_dir: ""
rng_seed: 0
mode: train
resume: False
num_gpu: 1

logger:
  name: "wandb"

debug:
  flag: False
  debug_size: 20

trainer: BaseTrainer

use_cot: False
dataset_wrapper: GPTWrapper
dataloader:
  # This is a per-gpu batch_size
  batch_size: 128
  num_workers: 2

solver:
  gradient_accumulation_steps: 1
  grad_norm: 5.0
  epochs: 10
  epochs_per_eval: 1
  epochs_per_save: 0
  optim:
    name: AdamW
    args:
      lr: 1e-4
      betas: [0.9, 0.98]
  sched:
    name: warmup_cosine
    args:
      warmup_steps: 100

eval:
  name: BaseEvaluator